{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"simple_example\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    # commutative and associative!\n",
    "    return a + b\n",
    "\n",
    "rdd = spark_context.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)\n",
    "\n",
    "result = rdd.map(lambda x: x * 2)\\\n",
    "            .reduce(add)\n",
    "\n",
    "print(result)\n",
    "\n",
    "#See: http://spark.apache.org/docs/2.3.0/api/python/pyspark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores for another application!\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1862234"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------------A.1.1----------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"simple_example\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "#Cashing text files\n",
    "swedish_txt = spark_context.textFile('txts/europarl-v7.sv-en.sv').cache()\n",
    "english_txt = spark_context.textFile('txts/europarl-v7.sv-en.en').cache()\n",
    "\n",
    "english_txt.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "1862234\n",
      "Swedish:\n",
      "1862234\n"
     ]
    }
   ],
   "source": [
    "#---------------A.1.2----------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"simple_example\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "#Cashing text files\n",
    "swedish_txt = spark_context.textFile('txts/europarl-v7.sv-en.sv').cache()\n",
    "english_txt = spark_context.textFile('txts/europarl-v7.sv-en.en').cache()\n",
    "\n",
    "print(\"English:\")\n",
    "print(english_txt.count())\n",
    "print(\"Swedish:\")\n",
    "print(swedish_txt.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "1862234\n",
      "Swedish:\n",
      "1862234\n",
      "They have the same amount!!!\n"
     ]
    }
   ],
   "source": [
    "#---------------A.1.3----------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"simple_example\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "#Cashing text files\n",
    "swedish_txt = spark_context.textFile('txts/europarl-v7.sv-en.sv').cache()\n",
    "english_txt = spark_context.textFile('txts/europarl-v7.sv-en.en').cache()\n",
    "\n",
    "print(\"English:\")\n",
    "print(english_txt.count())\n",
    "print(\"Swedish:\")\n",
    "print(swedish_txt.count())\n",
    "\n",
    "if(english_txt.count()==swedish_txt.count()):\n",
    "    print(\"They have the same amount!!!\")\n",
    "else:\n",
    "    print(\"They DO NOT have the same amount!!!\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "9\n",
      "Swedish:\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#---------------A.1.4----------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"simple_example\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "#Cashing text files\n",
    "swedish_txt = spark_context.textFile('txts/europarl-v7.sv-en.sv').cache()\n",
    "english_txt = spark_context.textFile('txts/europarl-v7.sv-en.en').cache()\n",
    "\n",
    "print(\"English:\")\n",
    "print(english_txt.getNumPartitions())\n",
    "print(\"Swedish:\")\n",
    "print(swedish_txt.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resumption of the session',\n",
       " 'i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
       " \"although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\",\n",
       " 'you have requested a debate on this subject in the course of the next few days, during this part-session.',\n",
       " \"in the meantime, i should like to observe a minute' s silence, as a number of members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the european union.\",\n",
       " \"please rise, then, for this minute' s silence.\",\n",
       " \"(the house rose and observed a minute' s silence)\",\n",
       " 'madam president, on a point of order.',\n",
       " 'you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka.',\n",
       " 'one of the people assassinated very recently in sri lanka was mr kumar ponnambalam, who had visited the european parliament just a few months ago.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------------A.2.1 and A.2.2----------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"simple_example\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "#Cashing text files\n",
    "swedish_txt = spark_context.textFile('txts/europarl-v7.sv-en.sv').cache()\n",
    "english_txt = spark_context.textFile('txts/europarl-v7.sv-en.en').cache()\n",
    "\n",
    "english_txt\\\n",
    ".map(lambda x: x.lower())\\\n",
    ".map(lambda w: w.strip()).cache().take(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They have the same amount!!!\n",
      "1862234\n",
      "1862234\n"
     ]
    }
   ],
   "source": [
    "#---------------A.2.3----------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"simple_example\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "#Cashing text files\n",
    "swedish_txt = spark_context.textFile('txts/europarl-v7.sv-en.sv').cache()\n",
    "english_txt = spark_context.textFile('txts/europarl-v7.sv-en.en').cache()\n",
    "\n",
    "en_count = english_txt\\\n",
    ".map(lambda x: x.lower())\\\n",
    ".map(lambda w: w.strip()).count()\n",
    "\n",
    "sv_count = swedish_txt\\\n",
    ".map(lambda x: x.lower())\\\n",
    ".map(lambda w: w.strip()).count()\n",
    "\n",
    "if(en_count==sv_count):\n",
    "    print(\"They have the same amount!!!\")\n",
    "    print(en_count)\n",
    "    print(sv_count)\n",
    "else:\n",
    "    print(\"They DO NOT have the same amount!!!\")\n",
    "    print(en_count)\n",
    "    print(sv_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3498452),\n",
       " ('of', 1659758),\n",
       " ('to', 1539760),\n",
       " ('and', 1288402),\n",
       " ('in', 1085994),\n",
       " ('that', 797519),\n",
       " ('a', 773522),\n",
       " ('is', 758050),\n",
       " ('for', 534242),\n",
       " ('we', 522851)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------------A.3.1-first part----------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"simple_example\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "#Cashing text files\n",
    "swedish_txt = spark_context.textFile('txts/europarl-v7.sv-en.sv').cache()\n",
    "english_txt = spark_context.textFile('txts/europarl-v7.sv-en.en').cache()\n",
    "\n",
    "english_txt\\\n",
    ".map(lambda x: x.lower())\\\n",
    ".flatMap(lambda t: t.split(' '))\\\n",
    ".flatMap(lambda w: w.split('\\n'))\\\n",
    ".map(lambda w: w.strip()).cache()\\\n",
    ".map(lambda w: (w,1))\\\n",
    ".reduceByKey(add)\\\n",
    ".takeOrdered(10, key=lambda x: -x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('att', 1706293),\n",
       " ('och', 1344830),\n",
       " ('i', 1050774),\n",
       " ('det', 924868),\n",
       " ('som', 913276),\n",
       " ('för', 908680),\n",
       " ('av', 738068),\n",
       " ('är', 694381),\n",
       " ('en', 620310),\n",
       " ('vi', 539797)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------------A.3.1-second part----------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"simple_example\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "#Cashing text files\n",
    "swedish_txt = spark_context.textFile('txts/europarl-v7.sv-en.sv').cache()\n",
    "english_txt = spark_context.textFile('txts/europarl-v7.sv-en.en').cache()\n",
    "\n",
    "swedish_txt\\\n",
    ".map(lambda x: x.lower())\\\n",
    ".flatMap(lambda t: t.split(' '))\\\n",
    ".flatMap(lambda w: w.split('\\n'))\\\n",
    ".map(lambda w: w.strip()).cache()\\\n",
    ".map(lambda w: (w,1))\\\n",
    ".reduceByKey(add)\\\n",
    ".takeOrdered(10, key=lambda x: -x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#---------------A.3.2----------------\n",
    "\n",
    "Since the english 10 most commonly mentioned words are:\n",
    "    \n",
    "[('the', 3498452),\n",
    " ('of', 1659758),\n",
    " ('to', 1539760),\n",
    " ('and', 1288402),\n",
    " ('in', 1085994),\n",
    " ('that', 797519),\n",
    " ('a', 773522),\n",
    " ('is', 758050),\n",
    " ('for', 534242),\n",
    " ('we', 522851)]\n",
    "    \n",
    "and the swedish 10 most commonly mentioned words are:\n",
    "    \n",
    "[('att', 1706293),\n",
    " ('och', 1344830),\n",
    " ('i', 1050774),\n",
    " ('det', 924868),\n",
    " ('som', 913276),\n",
    " ('för', 908680),\n",
    " ('av', 738068),\n",
    " ('är', 694381),\n",
    " ('en', 620310),\n",
    " ('vi', 539797)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------A.4.1----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: Record the engish line numbers associated with each line (hint: ZipWithIndex())\n",
    "#2: Swap the key and value - so that the line number is the key\n",
    "import re\n",
    "\n",
    "en_rdd = english_txt\\\n",
    ".map(lambda x: x.lower())\\\n",
    ".map(lambda x: re.sub(r'\\W+', ' ', x))\\\n",
    ".map(lambda x: re.sub(r'[0-9]+', '', x))\\\n",
    ".flatMap(lambda w: w.split('\\n'))\\\n",
    ".map(lambda w: w.strip())\\\n",
    ".zipWithIndex()\\\n",
    ".map(lambda z: (z[1], z[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: Record the swedish line numbers associated with each line (hint: ZipWithIndex())\n",
    "#2: Swap the key and value - so that the line number is the key\n",
    "\n",
    "sv_rdd = swedish_txt\\\n",
    ".map(lambda x: x.lower())\\\n",
    ".map(lambda x: re.sub(r'\\W+', ' ', x))\\\n",
    ".map(lambda x: re.sub(r'[0-9]+', '', x))\\\n",
    ".flatMap(lambda w: w.split('\\n'))\\\n",
    ".map(lambda w: w.strip())\\\n",
    ".zipWithIndex()\\\n",
    ".map(lambda z: (z[1], z[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['applause'], ['applåder']),\n",
       " (['president'], ['talmannen']),\n",
       " (['applause'], ['applåder']),\n",
       " (['loud', 'applause'], ['livliga', 'applåder']),\n",
       " (['sustained', 'applause'], ['ihållande', 'applåder']),\n",
       " (['when'], ['när']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['laughter'], ['skratt']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['president'], ['talmannen']),\n",
       " (['reg'], ['reg']),\n",
       " (['vote'], ['omröstning']),\n",
       " (['c'], ['c']),\n",
       " (['laughter'], ['skratt']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['bravo'], ['bravo']),\n",
       " (['laughter'], ['skratt']),\n",
       " (['szabolcs', 'fazakas'], ['szabolcs', 'fazakas']),\n",
       " (['rwanda', 'burundi'], ['rwanda', 'burundi']),\n",
       " (['applause'], ['applåder']),\n",
       " (['vote'], ['omröstning']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['why'], ['varför']),\n",
       " (['rapporteur'], ['föredragande']),\n",
       " (['applause'], ['applåder']),\n",
       " (['no'], ['nej']),\n",
       " (['applause'], ['applåder']),\n",
       " (['subject', 'cloning'], ['angående', 'kloning']),\n",
       " (['obviously', 'not'], ['självklart', 'inte']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['eurodac'], ['eurodac']),\n",
       " (['on', 'paragraph'], ['angående', 'punkt']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['igc'], ['regeringskonferensen']),\n",
       " (['applause'], ['applåder']),\n",
       " (['report', 'staes'], ['betänkande', 'staes']),\n",
       " (['no'], ['nej']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['why'], ['varför']),\n",
       " (['amazing'], ['häpnadsväckande']),\n",
       " (['mixed', 'reactions'], ['blandade', 'reaktioner']),\n",
       " (['votes'], ['omröstning']),\n",
       " (['applause'], ['applåder']),\n",
       " (['wednesday'], ['onsdagen']),\n",
       " (['applause'], ['applåder']),\n",
       " (['no', 'sorry'], ['nej', 'tyvärr']),\n",
       " (['decision'], ['beslut']),\n",
       " (['heckling'], ['häcklande']),\n",
       " (['applause'], ['applåder']),\n",
       " (['part', 'two'], ['del', 'ii']),\n",
       " (['and', 'why'], ['varför', 'det']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['why', 'wait'], ['varför', 'vänta']),\n",
       " (['applause'], ['applåder']),\n",
       " (['why'], ['varför']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['nothing'], ['ingenting']),\n",
       " (['ukraine'], ['ukraina']),\n",
       " (['report', 'itälä'], ['betänkande', 'itälä']),\n",
       " (['yes'], ['ja']),\n",
       " (['wallis', 'report'], ['betänkande', 'wallis']),\n",
       " (['guinea', 'vote'], ['guinea', 'omröstning']),\n",
       " (['bolkestein', 'commission'], ['bolkestein', 'kommissionen']),\n",
       " (['papadimoulis', 'report'], ['betänkande', 'papadimoulis']),\n",
       " (['tourism'], ['turism']),\n",
       " (['how'], ['hur']),\n",
       " (['report', 'roithová'], ['betänkande', 'roithová']),\n",
       " (['yes'], ['ja']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['part', 'i'], ['första', 'delen']),\n",
       " (['standing', 'ovation'], ['stående', 'ovationer']),\n",
       " (['surely', 'not'], ['självklart', 'inte']),\n",
       " (['applause'], ['applåder']),\n",
       " (['applause'], ['applåder']),\n",
       " (['subject', 'cyprus'], ['angående', 'cypern']),\n",
       " (['not', 'otherwise'], ['annars', 'inte']),\n",
       " (['applause'], ['applåder']),\n",
       " (['report', 'ries'], ['betänkande', 'ries']),\n",
       " (['applause'], ['applåder'])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3: Match the lines in each corpus, so you have pairs of matching lines. (hint: join())\n",
    "#find suitable sentences/lines\n",
    "\n",
    "union_rdd = en_rdd.join(sv_rdd)\n",
    "union_rdd.cache()\n",
    "\n",
    "#Pre-process the lines, to split the words as before, but don’t flatten yet.\n",
    "#Filter to exclude line pairs that have an empty/missing “corresponding” sentence.\n",
    "#Filter to leave only pairs of sentences with a small number of words per sentence (this\n",
    "#should give a more reliable translation (you can experiment).\n",
    "#Filter to leave only pairs of sentences with the same number of words in each sentence.\n",
    "maximum_len = 3\n",
    "result = union_rdd\\\n",
    ".map(lambda t: (t[1][0].split(' '),t[1][1].split(' ')))\\\n",
    ".filter(lambda x: x[0][0] != \"\")\\\n",
    ".filter(lambda x: len(x[0])<maximum_len)\\\n",
    ".filter(lambda t: len(t[0])==len(t[1]))\\\n",
    "\n",
    "result.take(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#---------------A.4.1----------------\n",
    "\n",
    "Yes, they seem reasonable. Many applause but that is most probably because of taking lines with less than 3 words and also just returning the first 100, maybe a bit to short but more secure. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
